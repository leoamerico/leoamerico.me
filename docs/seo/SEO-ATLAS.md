# SEO Atlas ‚Äî leoamerico.me

> SEO como sistema governado. Tr√™s planos. Cinco vis√µes. Gates bin√°rios que impedem regress√£o.

---

## Modelo mental

SEO n√£o √© "achismo de keywords". √â **observabilidade sobre o que o Google pode descobrir, entender e confiar**. O mesmo princ√≠pio do ESA se aplica:

| Plano | Pergunta | Fontes |
|-------|----------|--------|
| **Discovery** üîµ | Crawlers *encontram*? | `robots.ts`, `sitemap.ts`, status HTTP, canonical |
| **Relevance** üü£ | Crawlers *entendem*? | `metadata`, OG tags, `schema.org`, conte√∫do por rota |
| **Performance** üü° | Crawlers *confiam*? | Core Web Vitals, payload JS, headers, a11y |

**Regra invariante:** toda a√ß√£o de SEO declara em qual plano atua.

---

## As 5 vis√µes

### V1 ‚Äî Mapa de Rotas Index√°veis

Pergunta: *"o que o Google pode indexar?"*

- Lista de rotas com status HTTP, canonical, `index/noindex`, `lastmod` do sitemap
- Remove o problema "tem p√°gina mas n√£o indexa"

**Acesso:** `/atlas/seo` ‚Üí tab "V1 ‚Äî Rotas"

### V2 ‚Äî Matriz de Metadados

Pergunta: *"toda rota tem o m√≠nimo correto?"*

Linhas: rotas ¬∑ Colunas: `title`, `desc`, `canonical`, `og:title`, `og:desc`, `og:img`, `tw:img`, `robots`, `lang`, `schema.org`

C√©lulas: ‚úÖ/‚ö†Ô∏è/‚ùå ¬∑ Drill-down: aponta arquivo de origem.

√â o equivalente SEO da [Coverage Matrix ESA](/atlas/matrix).

**Acesso:** `/atlas/seo` ‚Üí tab "V2 ‚Äî Metadados"

### V3 ‚Äî Cobertura de Conte√∫do por Persona/Intent

Pergunta: *"o conte√∫do atende inten√ß√£o e n√£o compete consigo?"*

#### Dual-track

| Track | Fonte | Disponibilidade |
|---|---|---|
| **Static** | `lib/constants.ts` ‚Üí `lib/seo/buildV3Static.ts` | Atlas UI, sempre dispon√≠vel (serverless) |
| **Live** | HTML fetch via `scripts/seo/seo-v3-coverage.mjs` | CI/CD, requer servidor rodando |

#### Como rodar o script live

```bash
bun run build && bun run start -p 3000 &
bun run seo:v3          # JSON em reports/seo/seo-v3-coverage.json
bun run seo:v3:md       # Markdown em reports/seo/seo-v3-coverage.md
# Config:
npm run seo:v3 -- --url http://localhost:3000 --threshold 300
```

> `reports/` est√° no `.gitignore` ‚Äî os arquivos s√£o artefatos de CI, n√£o commitados.

#### Thresholds

| Track | Thin P1 | Thin P2 |
|---|---|---|
| Static (fragmentos de texto) | < 60 palavras | < 120 palavras && sem H2 |
| Live (HTML body completo) | < threshold (padr√£o 250w) | < threshold && sem H1 |

#### Personas reconhecidas

`prefeito ¬∑ secretario ¬∑ procurador ¬∑ auditor ¬∑ controlador ¬∑ fiscal ¬∑ cidadao ¬∑ empresario ¬∑ gestor ¬∑ tech ¬∑ geral`

#### Intents reconhecidos

`hero ¬∑ sobre ¬∑ credibilidade ¬∑ produto-grp ¬∑ produto-erp ¬∑ compliance ¬∑ governanca ¬∑ stack ¬∑ capacidades ¬∑ contato ¬∑ overview`

#### Findings gerados

| Tipo | Severidade | Crit√©rio |
|---|---|---|
| `thin-content` | P1/P2 | word count abaixo do threshold |
| `cannibalization` | P0/P1 | H1 duplicado, assinatura id√™ntica, H2 sobreposi√ß√£o ‚â• 2 |
| `heading-drift` | P1 | headings n√£o refletem a persona/intent da unidade |
| `promise-drift` | P1 | t√≠tulo promete X, conte√∫do entrega Y |

#### Limita√ß√µes

- **SPA de rota √∫nica:** o site tem apenas `/` index√°vel; as √¢ncoras (`#sobre`, `#audit`, etc.) s√£o se√ß√µes da mesma p√°gina HTML, n√£o p√°ginas separadas. O script trata cada √¢ncora como ContentUnit independente.
- **JS-rendered content:** o script usa fetch est√°tico (n√£o Puppeteer); conte√∫do renderizado via JS n√£o √© capturado pelo track live.
- **Word counts est√°ticos:** os fragmentos em `lib/constants.ts` s√£o menores que o HTML completo renderizado ‚Äî threshold de 60w √© calibrado para isso.

### V4 ‚Äî Sa√∫de T√©cnica / CWV *(backlog)*

Pergunta: *"o site √© r√°pido e est√°vel o suficiente para rankear?"*

LCP / CLS / INP ¬∑ payload JS por rota ¬∑ imagens ¬∑ caching ¬∑ headers.

### V5 ‚Äî Story Mode

Pergunta: *"como explicar o estado do SEO em 90s/5min/15min?"*

- **Executivo (90s):** indexa√ß√£o ok? p√°ginas ok? vitals ok?
- **Editor (5min):** quais p√°ginas precisam de conte√∫do e qual inten√ß√£o?
- **Engenheiro (15min):** arquivos a corrigir + gates que impedem regress√£o

**Acesso:** `/atlas/seo` ‚Üí tab "V5 ‚Äî Story"

---

## Gates bin√°rios E-SEO-1..5

| Gate | Nome | Plano | Falha se |
|------|------|-------|----------|
| **E-SEO-1** | Robots & Sitemap coerentes | Discovery | `sitemap` n√£o referenciado em `robots.ts`; sitemap sem URLs; rota no sitemap bloqueada por `Disallow` |
| **E-SEO-2** | Toda rota index√°vel tem metadata m√≠nima | Relevance | rota 200 + index√°vel sem `title`, `description`, `canonical` ou `og:image` |
| **E-SEO-3** | Canonical √∫nico e sem auto-conflito | Relevance | rota index√°vel com canonical apontando para URL diferente sem redirect expl√≠cito |
| **E-SEO-4** | Nenhuma thin page em produ√ß√£o | Relevance | p√°gina index√°vel com <200 palavras e sem `schema.org` |
| **E-SEO-5** | CWV baseline (Lighthouse CI) | Performance | Lighthouse seo <90, a11y <90, performance <70 |

---

## Fontes can√¥nicas (Truth Sources)

```
app/robots.ts           ‚Üí regras de disallow + refer√™ncia ao sitemap
app/sitemap.ts          ‚Üí lista de rotas index√°veis + lastmod/priority
app/layout.tsx          ‚Üí metadata raiz: title, description, OG, Twitter, JSON-LD
app/(*/page.tsx)        ‚Üí metadata por rota (override do layout)
lib/structured-data.ts  ‚Üí schema.org Person + WebSite
lib/constants.ts        ‚Üí SITE (url, title, description, ogImage, locale)
next.config.mjs         ‚Üí headers de seguran√ßa/cache
public/robots.txt       ‚Üí espelho est√°tico do app/robots.ts
```

---

## Implementa√ß√£o

### Scripts

```bash
# Rodar gates localmente (servidor deve estar ativo na porta 3000)
bun run seo:gates

# Output JSON para CI
bun run seo:gates --json > reports/seo/seo-gates.json

# Contra staging/produ√ß√£o
bun run seo:gates --url https://leoamerico.me
```

Adicione ao `package.json`:
```json
"seo:gates": "node scripts/seo/seo-gates.mjs"
```

### CI (automatizado)

`.github/workflows/seo-gates.yml` ‚Äî dois jobs:

1. **`seo-metadata`** ‚Äî E-SEO-1..4: build ‚Üí start server ‚Üí `seo-gates.mjs` ‚Üí upload artefato
2. **`lighthouse-ci`** ‚Äî E-SEO-5: Lighthouse CI com thresholds (seo‚â•90, a11y‚â•90, perf‚â•70)

Trigger: `push main`, `PR` em arquivos SEO, `cron` semanal.

---

## Estado atual

| Gate | Status | Detalhe |
|------|--------|---------|
| E-SEO-1 | ‚úÖ PASS | `robots.ts` referencia sitemap; sitemap inclui rota raiz |
| E-SEO-2 | ‚ö†Ô∏è WARN | `title` de 24 chars (ideal ‚â•30); `description` ok (143 chars) |
| E-SEO-3 | ‚úÖ PASS | Canonical da home = `https://leoamerico.me/` sem conflito |
| E-SEO-4 | ‚úÖ PASS | Home tem schema.org Person + WebSite + rich content |
| E-SEO-5 | ‚ö†Ô∏è WARN | Workflow presente; relat√≥rio Lighthouse n√£o gerado ainda |

**A√ß√£o priorit√°ria:** ajustar `title` de `"Leo Am√©rico ‚Äî ERP ¬∑ GRP"` (24 chars) para ‚â•30 chars.  
**Sugest√£o:** `"Leo Am√©rico ‚Äî Arquitetura ERP ¬∑ GRP"` (37 chars).

---

## Roadmap

- [ ] **E-SEO-2** ‚Äî Corrigir `SITE.title` para ‚â•30 chars
- [ ] **E-SEO-5** ‚Äî Gerar primeiro relat√≥rio Lighthouse e ajustar thresholds reais
- [ ] **V3** ‚Äî Implementar an√°lise de inten√ß√£o/persona (ap√≥s definir cluster de personas para GRP/ERP)
- [ ] **V4** ‚Äî Timeline de CWV com hist√≥rico por commit (integrar LHCI com upload para servidor)
- [ ] **Search Console** ‚Äî Integrar cobertura real de indexa√ß√£o via API (verdade externa)
